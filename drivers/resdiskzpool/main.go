//go:build linux
// +build linux

package resdiskzpool

import (
	"context"
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"time"

	"github.com/pkg/errors"
	"github.com/rs/zerolog"
	"opensvc.com/opensvc/core/actionrollback"
	"opensvc.com/opensvc/core/drivergroup"
	"opensvc.com/opensvc/core/keywords"
	"opensvc.com/opensvc/core/manifest"
	"opensvc.com/opensvc/core/provisioned"
	"opensvc.com/opensvc/core/rawconfig"
	"opensvc.com/opensvc/core/resource"
	"opensvc.com/opensvc/core/status"
	"opensvc.com/opensvc/drivers/resdisk"
	"opensvc.com/opensvc/util/command"
	"opensvc.com/opensvc/util/converters"
	"opensvc.com/opensvc/util/device"
	"opensvc.com/opensvc/util/file"
	"opensvc.com/opensvc/util/zfs"
)

const (
	driverGroup = drivergroup.Disk
	driverName  = "zpool"
)

type (
	T struct {
		resdisk.T
		Name          string   `json:"name"`
		Size          string   `json:"size"`
		CreateOptions []string `json:"create_options"`
		VDev          []string `json:"vdev"`
		Multihost     string   `json:"multihost"`
		Zone          string   `json:"zone"`
	}
)

func init() {
	resource.Register(driverGroup, driverName, New)
}

func New() resource.Driver {
	t := &T{}
	return t
}

// Manifest exposes to the core the input expected by the driver.
func (t T) Manifest() *manifest.T {
	m := manifest.New(driverGroup, driverName, t)
	m.AddKeyword(resdisk.BaseKeywords...)
	m.AddKeyword([]keywords.Keyword{
		{
			Option:   "name",
			Attr:     "Name",
			Required: true,
			Scopable: true,
			Text:     "The name of the zfs pool.",
			Example:  "tank",
			Aliases:  []string{"poolname"},
		},
		{
			Option:    "multihost",
			Attr:      "Multihost",
			Scopable:  true,
			Converter: converters.Tristate,
			Text:      "If set to ``true``, sets zfs property ``multihost=on`` on start if not already set. This requires all nodes to be booted with a /etc/hostid installed, preferrably generated by the zgenhostid command. If set to ``false``, sets the zfs property ``multihost=off``. If left empty, the current multihost property is left unchanged.",
			Example:   "yes",
		},
		{
			Option:       "vdev",
			Attr:         "VDev",
			Scopable:     true,
			Converter:    converters.List,
			Provisioning: true,
			Text:         "The vdev list, including optional parity keywords, as would be passed to zpool create.",
			Example:      "/dev/mapper/23 /dev/mapper/24",
		},
		{
			Option:       "create_options",
			Attr:         "CreateOptions",
			Converter:    converters.Shlex,
			Scopable:     true,
			Provisioning: true,
			Text:         "The zone name the zpool refers to. If set, the zpool is activated in the zone context.",
			Example:      "-O dedup=on",
		},
		{
			Option:   "zone",
			Attr:     "Zone",
			Scopable: true,
			Text:     "The zone name the zpool refers to. If set, the zpool is activated in the zone context.",
		},
	}...)
	return m
}

func (t T) subDevsFilePath() string {
	return filepath.Join(t.VarDir(), "sub_devs")
}

func (t T) ToSync() []string {
	return []string{
		t.subDevsFilePath(),
	}
}

func (t T) PreSync() error {
	_, err := t.updateSubDevsFile()
	return err
}

func (t T) updateSubDevsFile() ([]string, error) {
	if v, err := t.hasIt(); err != nil {
		return nil, err
	} else if !v {
		return nil, nil
	}
	l, err := t.listPoolVDevs()
	if err != nil {
		return nil, errors.Wrap(err, "update sub devs cache")
	}
	if err := t.writeSubDevsFile(l); err != nil {
		return l, err
	}
	return l, nil
}

func (t T) writeSubDevsFile(l []string) error {
	path := t.subDevsFilePath()
	f, err := os.Open(path)
	if err != nil {
		return errors.Wrap(err, "write sub devs cache")
	}
	defer f.Close()
	enc := json.NewEncoder(f)
	err = enc.Encode(l)
	if err != nil {
		return errors.Wrap(err, "write sub devs cache")
	}
	return nil
}

func (t T) loadSubDevsFile() ([]string, error) {
	path := t.subDevsFilePath()
	l := make([]string, 0)
	f, err := os.Open(path)
	if err != nil {
		return nil, errors.Wrap(err, "load sub devs cache")
	}
	defer f.Close()
	dec := json.NewDecoder(f)
	err = dec.Decode(l)
	if err != nil {
		return nil, errors.Wrap(err, "load sub devs cache")
	}
	return l, nil
}

func (t T) hasIt() (bool, error) {
	return t.pool().Exists()
}

func (t T) listPoolVDevs() ([]string, error) {
	panic("todo")
	l := make([]string, 0)
	return l, nil
}

func (t T) listPoolZDevs() ([]string, error) {
	panic("todo")
	l := make([]string, 0)
	return l, nil
}

func (t T) setMultihost() error {
	if t.Multihost == "" {
		return nil
	}
	var value string
	switch t.Multihost {
	case "true":
		value = "on"
	case "false":
		value = "off"
	}
	pool := t.pool()
	current, err := pool.GetProperty("multihost")
	if err != nil {
		return err
	}
	if current == value {
		t.Log().Info().Msgf("multihost property is already %s", value)
		return nil
	}
	return t.pool().SetProperty("multihost", value)
}

func (t T) Start(ctx context.Context) error {
	if v, err := t.isUp(); err != nil {
		return err
	} else if v {
		t.Log().Info().Msgf("%s is already up", t.Label())
		return nil
	}
	if err := t.doHostID(); err != nil {
		return err
	}
	if err := t.poolImport(); err != nil {
		return err
	}
	if err := t.setMultihost(); err != nil {
		return err
	}
	actionrollback.Register(ctx, func() error {
		return t.poolExport()
	})
	return nil
}

func (t T) Info() map[string]string {
	m := make(map[string]string)
	m["name"] = t.Name
	return m
}

func (t T) doHostID() error {
	switch t.Multihost {
	case "", "false":
		return nil
	default:
		return t.genHostID()
	}
}

func (t T) genHostID() error {
	if file.Exists("/etc/hostid") {
		return nil
	}
	p, err := exec.LookPath("zgenhostid")
	if err != nil {
		t.Log().Warn().Msg("/etc/hostid does not exist and zgenhostid is not installed")
		return nil
	}
	cmd := command.New(
		command.WithName(p),
		command.WithLogger(t.Log()),
		command.WithCommandLogLevel(zerolog.InfoLevel),
		command.WithStdoutLogLevel(zerolog.InfoLevel),
		command.WithStderrLogLevel(zerolog.ErrorLevel),
	)
	return cmd.Run()
}

func (t T) Stop(ctx context.Context) error {
	if v, err := t.isUp(); err != nil {
		return err
	} else if !v {
		t.Log().Info().Msgf("%s is already down", t.Label())
		return nil
	}
	if err := t.poolExport(); err != nil {
		return err
	}
	return nil
}

func (t T) isUp() (bool, error) {
	pool := t.pool()
	if v, err := t.hasIt(); err != nil {
		return false, err
	} else if !v {
		return false, nil
	}
	data, err := pool.Status(zfs.PoolStatusWithVerbose())
	if err != nil {
		return false, err
	}
	switch data.State {
	case "ONLINE":
		return true, nil
	case "SUSPENDED", "DEGRADED":
		t.StatusLog().Warn(strings.ToLower(data.State))
		return false, nil
	default:
		return false, nil
	}
}

func (t *T) Status(ctx context.Context) status.T {
	if v, err := t.isUp(); err != nil {
		t.StatusLog().Error("%s", err)
		return status.Undef
	} else if v {
		return status.Up
	}
	return status.Down
}

func (t T) Label() string {
	return t.Name
}

//
// poolImport imports the pool.
// 1/ try using a dev list cache, which is fastest
// 2/ fallback without dev list cache
//
// Parallel import can fail on Solaris 11.4, with a "no such
// pool available" error. Retry in this case, if we confirm the
// pool exists.
//
func (t T) poolImport() error {
	var err error
	for i := 0; i < 10; i += 1 {
		err = t.poolImportTryDevice()
		if err == nil {
			return nil
		}
		time.Sleep(time.Second)
	}
	return err
}

func (t T) poolImportCacheFile() string {
	return filepath.Join(rawconfig.Node.Paths.Var, "zpool.cache")
}

func (t T) poolImportDeviceDir() string {
	return filepath.Join(t.VarDir(), "dev", "dsk")
}

func (t T) poolImportTryDevice() error {
	if err := t.poolImportWithDevice(); err == nil {
		return nil
	}
	return t.poolImportWithoutDevice()
}

func (t T) poolImportWithoutDevice() error {
	c := t.poolImportCacheFile()
	return t.pool().Import(
		zfs.PoolImportWithForce(),
		zfs.PoolImportWithOption("cachefile", c),
	)
}

func (t T) poolImportWithDevice() error {
	d := t.poolImportDeviceDir()
	if !file.Exists(d) {
		return fmt.Errorf("%s does not exist", d)
	}
	c := t.poolImportCacheFile()
	return t.pool().Import(
		zfs.PoolImportWithForce(),
		zfs.PoolImportWithOption("cachefile", c),
		zfs.PoolImportWithDevice(d),
	)
}

func (t T) poolExport() error {
	pool := t.pool()
	if err := pool.Export(); err == nil {
		return nil
	}
	return pool.Export(zfs.PoolExportWithForce())
}

func (t T) createPool() error {
	panic("todo")
	return nil
}

func (t T) destroyPool() error {
	panic("todo")
	return nil
}

func (t T) pool() *zfs.Pool {
	return &zfs.Pool{
		Name: t.Name,
		Log:  t.Log(),
	}
}

func (t T) ProvisionLeader(ctx context.Context) error {
	if v, err := t.hasIt(); err != nil {
		return err
	} else if v {
		t.Log().Info().Msgf("%s is already provisioned", t.Name)
		return nil
	}
	return t.createPool()
}

func (t T) UnprovisionLeader(ctx context.Context) error {
	if v, err := t.hasIt(); err != nil {
		return err
	} else if !v {
		t.Log().Info().Msgf("%s is already unprovisioned", t.Name)
		return nil
	}
	return t.destroyPool()
}

func (t T) Provisioned() (provisioned.T, error) {
	if v, err := t.hasIt(); err != nil {
		return provisioned.Undef, err
	} else {
		return provisioned.FromBool(v), nil
	}
}

func (t T) ExposedDevices() []*device.T {
	if l, err := t.listPoolZDevs(); err == nil {
		return t.toDevices(l)
	} else {
		return []*device.T{}
	}
}

func (t T) SubDevices() []*device.T {
	if l, err := t.listPoolVDevs(); err == nil {
		return t.toDevices(l)
	} else {
		return []*device.T{}
	}
}

func (t T) toDevices(l []string) []*device.T {
	log := t.Log()
	devs := make([]*device.T, 0)
	for _, s := range l {
		dev := device.New(s, device.WithLogger(log))
		devs = append(devs, dev)
	}
	return devs
}
